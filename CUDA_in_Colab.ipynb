{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntOgKF_-y0SV",
        "outputId": "ecb79527-643f-4d3d-c2b2-09929c7f9edd"
      },
      "outputs": [],
      "source": [
        "# Install CUDA C++ plugin for Colab:\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVqh0baiO15d",
        "outputId": "c0957ba2-28e6-45ab-f6b2-788215e0268f"
      },
      "outputs": [],
      "source": [
        "# Detect selected GPU and its NVIDA architecture:\n",
        "import subprocess\n",
        "gpu_info = subprocess.getoutput(\"nvidia-smi --query-gpu=name,compute_cap --format=csv,noheader,nounits\")\n",
        "if \"not found\" in gpu_info.lower(): raise RuntimeError(\"Error: No GPU found. Please select a GPU runtime environment.\")\n",
        "gpu_name, compute_cap = map(str.strip, gpu_info.split(','))\n",
        "gpu_arch = f\"sm_{compute_cap.replace('.', '')}\"\n",
        "\n",
        "print(f\"{'GPU Name':<15}: {gpu_name}\")\n",
        "print(f\"{'Architecture':<15}: {gpu_arch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HW8N8RXzCTc",
        "outputId": "de2d81d7-0023-4a91-c331-9ab2906bf5e1"
      },
      "outputs": [],
      "source": [
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void hello_kernel() {\n",
        "    int blockId = blockIdx.x;\n",
        "    int threadId = threadIdx.x;\n",
        "    int globalId = threadId + blockId * blockDim.x;\n",
        "\n",
        "    printf(\"Hello from block %d, thread %d (global thread %d)\\n\", blockId, threadId, globalId);\n",
        "}\n",
        "int main() {\n",
        "    int numBlocks = 2;\n",
        "    int threadsPerBlock = 4;\n",
        "\n",
        "    hello_kernel<<<numBlocks, threadsPerBlock>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYBekTESahfS",
        "outputId": "cf6ecafe-0d64-44fa-e2b1-d5839c8e25cf"
      },
      "outputs": [],
      "source": [
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "// Kernel: cada bloque suma una parte del vector\n",
        "__global__ void sum_kernel(int *data, int *partial, int n) {\n",
        "    extern __shared__ int cache[];  // memoria compartida\n",
        "\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int cacheIndex = threadIdx.x;\n",
        "\n",
        "    int temp = 0;\n",
        "    // cada hilo suma su elemento si está dentro del rango\n",
        "    if (tid < n) {\n",
        "        temp = data[tid];\n",
        "    }\n",
        "\n",
        "    cache[cacheIndex] = temp;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reducción en memoria compartida (árbol binario)\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (cacheIndex < stride) {\n",
        "            cache[cacheIndex] += cache[cacheIndex + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // El primer hilo de cada bloque guarda el resultado parcial\n",
        "    if (cacheIndex == 0) {\n",
        "        partial[blockIdx.x] = cache[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1 << 16;  // tamaño del vector (65536)\n",
        "    const int THREADS = 256;\n",
        "    const int BLOCKS = (N + THREADS - 1) / THREADS;\n",
        "\n",
        "    int *h_data = new int[N];\n",
        "    for (int i = 0; i < N; i++) h_data[i] = 1; // inicializamos con 1's\n",
        "\n",
        "    int *d_data, *d_partial;\n",
        "    int *h_partial = new int[BLOCKS];\n",
        "\n",
        "    cudaMalloc(&d_data, N * sizeof(int));\n",
        "    cudaMalloc(&d_partial, BLOCKS * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_data, h_data, N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Ejecutar kernel\n",
        "    sum_kernel<<<BLOCKS, THREADS, THREADS * sizeof(int)>>>(d_data, d_partial, N);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Traer resultados parciales al host\n",
        "    cudaMemcpy(h_partial, d_partial, BLOCKS * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Acumular los parciales en CPU\n",
        "    long long total = 0;\n",
        "    for (int i = 0; i < BLOCKS; i++) total += h_partial[i];\n",
        "\n",
        "    printf(\"Suma total = %lld\\n\", total);\n",
        "\n",
        "    cudaFree(d_data);\n",
        "    cudaFree(d_partial);\n",
        "    delete[] h_data;\n",
        "    delete[] h_partial;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNdrwqOwa2qQ"
      },
      "outputs": [],
      "source": [
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "// Kernel: cada hilo calcula un elemento del vector resultado\n",
        "__global__ void vectorMatrixMul(const float *vec, const float *mat, float *res, int N, int M) {\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;  // índice de columna\n",
        "    if (col < M) {\n",
        "        float sum = 0.0f;\n",
        "        for (int row = 0; row < N; row++) {\n",
        "            sum += vec[row] * mat[row * M + col]; // mat en formato fila-major\n",
        "        }\n",
        "        res[col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 4;  // filas de la matriz (longitud del vector)\n",
        "    const int M = 5;  // columnas de la matriz\n",
        "    float h_vec[N] = {1, 2, 3, 4};  // vector\n",
        "    float h_mat[N * M];             // matriz NxM\n",
        "    float h_res[M];                 // resultado\n",
        "\n",
        "    // Inicializamos la matriz con valores sencillos\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < M; j++) {\n",
        "            h_mat[i * M + j] = (i + 1) * (j + 1); // ejemplo\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float *d_vec, *d_mat, *d_res;\n",
        "    cudaMalloc(&d_vec, N * sizeof(float));\n",
        "    cudaMalloc(&d_mat, N * M * sizeof(float));\n",
        "    cudaMalloc(&d_res, M * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_vec, h_vec, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_mat, h_mat, N * M * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Configuración de bloques/hilos\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocks = (M + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    // Lanzamos kernel\n",
        "    vectorMatrixMul<<<blocks, threadsPerBlock>>>(d_vec, d_mat, d_res, N, M);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copiamos el resultado al host\n",
        "    cudaMemcpy(h_res, d_res, M * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Mostramos resultado\n",
        "    printf(\"Resultado vector x matriz:\\n\");\n",
        "    for (int j = 0; j < M; j++) {\n",
        "        printf(\"%.1f \", h_res[j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    cudaFree(d_vec);\n",
        "    cudaFree(d_mat);\n",
        "    cudaFree(d_res);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
